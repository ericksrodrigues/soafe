# -*- coding: utf-8 -*-
"""SOM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mJe1Tmv6ZtIy0UGMYtFKce8Lw1EEVlza
"""

#!pip install SimpSOM

import pandas as pd

from sklearn.tree import  DecisionTreeClassifier,export_graphviz
from sklearn.model_selection import train_test_split
from sklearn import metrics
import numpy as np
import SimpSOM as sps
from sklearn.preprocessing import normalize
from sklearn.preprocessing import normalize
import random
from math import sqrt
from adaline_opt import adaline_func, generate_adaline_features

def map_class(row):
  return row
  # if(row == "tested_negative"):
  #   return 0
  # return 1


def concatenate_features(features_list):
  return np.hstack(features_list)
  # nf = None
  # for f in features_list:
  #     if nf is None:
  #       nf = np.copy(f)
  #     else:
  #       for i, nf_value in nf:
  #         nf[i] = np.concatenate((nf_value, f))
  # return nf

def desconstruction(
  model,
  target_function,
  data_path="./datasets/diabetes.csv",
  soms_config=[{
    "shape": (5,5),
    "learning_rate": 0.01,
    "iterations": 10000
  },
  ],
  adaline_outputs=10,
  adversarial_iteractions=10,
  subset_size=0.2,
  disableAdversarial=True
  ):
  df = pd.read_csv(data_path)
  df["class"] = df["class"].apply(map_class)
  df.head()
  g = df.groupby('class')
  df=g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))
  accuracy_array_normal = []
  accuracy_array_desc = []
  for x in range(1):
    
    X_train, X_test, y_train, y_test = train_test_split(df.drop('class',axis=1),df['class'],test_size=0.3,stratify=df['class'])
    X_train = normalize(X_train)
    X_test = normalize(X_test)
    X_train.shape,X_test.shape

    df_describe = pd.DataFrame(y_test)
    print(df_describe.describe())


    X_train_SOM_N, X_train_SOM, y_train_SOM_N, y_train_SOM = train_test_split(X_train,y_train,test_size=subset_size, stratify=y_train)

    X_train_SOM_N.shape, X_train_SOM.shape

    
  

    def automatic_transformation(dataframe,weights="flename_weights"):
      weights = np.load("./"+weights+".npy")
      new_data = []
      for data in dataframe:
        new_features = []
        for i,w in enumerate(weights):
          if i == 0:
            continue
          new_features.append(
             # sum([x*data[i] for i,x in enumerate(w)])
              sqrt(sum([(data[i] - x)**2 for i,x in enumerate(w)]))

          )
        new_data.append(np.concatenate(([],new_features)))
      return new_data
    nf = []
    for i,soms in enumerate(soms_config):
      net = sps.somNet(soms["shape"][0], soms["shape"][1], X_train_SOM, PBC=True)
      net.train(soms["learning_rate"], soms["iterations"])
      net.save('weights_' + str(i))
      nf.append(
        normalize(automatic_transformation(X_train_SOM_N,'weights_' + str(i)))
        )
    
    adaline_weight = None

    def generate_adversarial_new_features(feature_type, qt, inputs=[],outputs=10):
      new_feature = []
      if adaline_weight is None : 
        for x in range(0, qt):
          new_values = []
          for i in range(0,outputs):
            new_values.append(random.uniform(1,-1))
          new_feature.append(new_values)
      else:
        for x in inputs:
          new_feature.append(generate_adaline_features(x, adaline_weight))
      return new_feature

    if(not disableAdversarial):
      for i in range(0, adversarial_iteractions):
        ##construir if
          X_train_pot, X_test_pot, y_train_pot, y_test_pot = train_test_split(X_train_SOM,y_train_SOM,test_size=0.3,stratify=y_train_SOM)

          nf_train_pot = [X_train_pot]
          nf_test_pot = [X_test_pot]

          for i in range(len(soms_config)):
            nf_train_pot.append(
              normalize(automatic_transformation(X_train_pot, 'weights_' + str(i)))
              )
            nf_test_pot.append(
              normalize(automatic_transformation(X_test_pot, 'weights_' + str(i)))
            )
          # print(nf_train_pot[0], X_train_pot[0])
          new_X_train_pot = concatenate_features(nf_train_pot)

          adversarial_train_features = generate_adversarial_new_features(feature_type="random", qt=len(X_train_pot),inputs=new_X_train_pot, outputs=adaline_outputs)
          new_X_train_pot = [np.concatenate((x,new_X_train_pot[i])) for i,x in enumerate(adversarial_train_features)]
          
          new_X_test_pot = concatenate_features(
            nf_test_pot
          )
          adversarial_test_features = generate_adversarial_new_features(feature_type="random", qt=len(X_test_pot),inputs=new_X_test_pot,outputs=adaline_outputs)
          new_X_test_pot = [np.concatenate((x,new_X_test_pot[i])) for i,x in enumerate(adversarial_test_features)]

        
          #from sklearn.neural_network import MLPClassifier
          ## Model to replace

          clf = model() ## RandomForestClassifier
          clf.fit(new_X_train_pot, y_train_pot)

          ## fim
          results = clf.predict(new_X_test_pot)
          old_accuracy = metrics.f1_score(y_test_pot,results)
          print("LEN", len(new_X_train_pot[0]))
          
          # def target_function(new_features):
          #   values_to_predict = []
          #   for i,nf in enumerate(new_features):
          #     values_to_predict.append(np.concatenate((nf,new_X_train_pot[i][10:])))
          #   results = clf.predict(values_to_predict)
          #   #print(metrics.f1_score(y_train_pot,results))
          #   return metrics.f1_score(y_train_pot,results)
          
          adaline_weight = adaline_func(
            [x[adaline_outputs:] for x in new_X_train_pot],
            target_function,
            w=adaline_weight,
            outputs=adaline_outputs,
            old_features=new_X_train_pot,
            test_values=y_train_pot,
            clf=clf
            )
          new_features = [generate_adaline_features(x[adaline_outputs:], adaline_weight) for x in new_X_test_pot]
          for i, n in enumerate(new_features):
            new_features[i] = np.concatenate((n,new_X_test_pot[i][adaline_outputs:]))
          new_X_test_pot = new_features
          results = clf.predict(new_X_test_pot)
          new_accuracy = metrics.f1_score(y_test_pot,results)
          print("old - ", old_accuracy, "new - ", new_accuracy)
     

    #nf_train = [X_train_SOM_N]
    ##Teste com features originais
    X_train_SOM_N = [x for x in X_train]
    y_train_SOM_N = [x for x in y_train]
    nf_train = [X_train_SOM_N]
    for i in range(len(soms_config)):
      nf_train.append(
        normalize(automatic_transformation(X_train_SOM_N, 'weights_' + str(i)))
        )
        
    # for i,x in enumerate(new_X_train_1):
    #   new_X_train.append(np.concatenate((X_train_SOM_N[i],x, new_X_train_2[i], new_X_train_3[i])))

    nf_test = [X_test]
    for i in range(0, len(soms_config)):
      nf_test.append(
        normalize(automatic_transformation(X_test, 'weights_' + str(i)))
        )
    new_X_train = concatenate_features(nf_train)
    new_X_test = concatenate_features(nf_test)
    # print(new_X_test[0])
    ##Potencialização
    if(not disableAdversarial):
      opt_new_X_train = [np.concatenate((
        generate_adaline_features(x, adaline_weight),x
        )) for x in new_X_train]

      opt_new_X_test = [np.concatenate((
        generate_adaline_features(x, adaline_weight),x
        )) for x in new_X_test]
    else:
      opt_new_X_train = [np.concatenate((
        [],x
        )) for x in new_X_train]

      opt_new_X_test = [np.concatenate((
        [],x
        )) for x in new_X_test]

    result_dict_normal = {}
    result_dict_desc = {}
    # clf = DecisionTreeClassifier()
    # clf = clf.fit(X_train,y_train)
    # clf.feature_importances_

    # results = clf.predict(X_test)
    # result_dict_normal["arvore"] = metrics.f1_score(y_test,results)


    # clf = DecisionTreeClassifier()
    # clf.fit(opt_new_X_train,y_train_SOM_N)

    # results = clf.predict(opt_new_X_test)
    # result_dict_desc["arvore"] = metrics.f1_score(y_test,results)

    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import GridSearchCV
    from sklearn.model_selection import KFold
    from sklearn.feature_selection import SelectFromModel
    fs = SelectFromModel(RandomForestClassifier().fit(X_train, y_train), prefit=True)
    X_train = fs.transform(X_train)

    clf = model()
    clf = GridSearchCV(estimator=clf, cv=KFold(10), param_grid={}, scoring='f1')
    clf.fit(X_train, y_train)
    X_test = fs.transform(X_test)
    results = clf.predict(X_test)
    result_not_opt = metrics.f1_score(y_test,results)

    fs = SelectFromModel(RandomForestClassifier().fit(opt_new_X_train, y_train_SOM_N), prefit=True)
    opt_new_X_train = fs.transform(opt_new_X_train)
    clf = model()
    clf = GridSearchCV(estimator=clf, cv=KFold(10), param_grid={}, scoring='f1')
    clf.fit(opt_new_X_train, y_train_SOM_N)
  
    #feature selection
    opt_new_X_test = fs.transform(opt_new_X_test)
    results = clf.predict(opt_new_X_test)
    result_dict_desc["randomforest"] = metrics.f1_score(y_test,results)
    print("F1 score", metrics.f1_score(y_test,results))
    print("Accuracy Score", metrics.accuracy_score(y_test,results))
    return (metrics.f1_score(y_test,results), result_not_opt)

    # from sklearn.neural_network import MLPClassifier
    # clf = MLPClassifier(solver='lbfgs', alpha=0.001,hidden_layer_sizes=(150,),, max_iter=10000)
    # clf.fit(X_train, y_train)
    # results = clf.predict(X_test)
    # result_dict_normal["rnn"] = metrics.f1_score(y_test,results)

    # clf = MLPClassifier(solver='lbfgs', alpha=0.001,hidden_layer_sizes=(150,),, max_iter=10000)
    # clf.fit(opt_new_X_train, y_train_SOM_N)
    # results = clf.predict(opt_new_X_test)
    # result_dict_desc["rnn"] = metrics.f1_score(y_test,results)
    # print("F1 score", metrics.f1_score(y_test,results))

    # from sklearn.neighbors import KNeighborsClassifier
    # clf=KNeighborsClassifier(n_neighbors=5)
    # clf.fit(X_train, y_train)
    # results = clf.predict(X_test)
    # result_dict_normal["knn"] = metrics.f1_score(y_test,results)

    # clf=KNeighborsClassifier(n_neighbors=5)
    # clf.fit(opt_new_X_train, y_train_SOM_N)
    # results = clf.predict(opt_new_X_test)
    # result_dict_desc["knn"] = metrics.f1_score(y_test,results)
    # print("Features normais")
    # print(result_dict_normal)
    # print("Features desconstruidas")
    # print(result_dict_desc)
    # accuracy_array_normal.append(result_dict_normal)
    # accuracy_array_desc.append(result_dict_desc)

# import json
# with open("normal.txt", 'w') as outfile:
#     json.dump(accuracy_array_normal,outfile)
# with open("desconstruida.txt", 'w') as outfile:
#     json.dump(accuracy_array_desc,outfile)